print(r'''
________             _______________             ______       
___  __ )_____ _________  ____/__  /___  _____  ____  /______ 
__  __  |  __ `/  ___/_  /_   __  /_  / / /_  |/_/_  / ___/ /_
_  /_/ // /_/ // /__ _  __/   _  / / /_/ /__>  < _  /__/_  __/
/_____/ \__,_/ \___/ /_/      /_/  \__,_/ /_/|_| /_____//_/   
                                                                                                            
BacFluxL+ v1.1.1


Livio Antonielli, May 2024
''')

# Modules:
import os
import sys
from urllib.parse import urlparse

# Configuration file (same directory as snakefile script):
configfile: 'config/config.yaml'

# Path to DBs and input/output directories in config file:
workdir: config['directories']['out_dir']
FASTQDIR = config['directories']['fastq_dir']
BLASTDB = config['directories']['blast_db']
DMNDDB = config['directories']['eggnog_db']
GTDBTKDB = config['directories']['gtdbtk_db']
BAKTADB = config['directories']['bakta_db']
PLATONDB = config['directories']['platon_db']

# AMR DBs (ABRicate):
DATABASES = ["argannot", "card", "ecoh", "ecoli_vf", "megares", "ncbi", "resfinder", "vfdb"]

# Hardware resources in config file:
CPUS = config['resources']['threads']
RAM = config['resources']['ram_gb']

# Sanity check of genus parameter:
if config['parameters'] is not None and "genus" in config['parameters']:
    genus = config['parameters']["genus"]
    if genus is not None and len(genus) > 0:
        print(f"The 'genus' parameter is specified in the config file with value: '{genus}'.")
    else:
        print("The 'genus' parameter value is not specified in the config file and will be inferred automatically.")
else:
    print("The 'genus' parameter is not present in the config file.")

# Sanity check of Medaka parameter:
if "medaka_model" in config['parameters']:
    model = config['parameters']["medaka_model"]
    if model is not None and len(model) > 0:
        print(f"The 'model' parameter of Medaka is specified in the config file with value: '{model}'.")
    else:
        print("The 'model' parameter of Medaka is not specified in the config file and the default will be used.")
else:
    sys.stderr.write("\nThe 'medaka_model' parameter is not present in the config file.\n")
    sys.exit(0)    

# Sanity check of checkv db link parameter:
global id
id = None

if config['links'] is not None and "checkv_link" in config['links']:
    link = config['links']["checkv_link"]
    if link is not None and len(link) > 0:
        path = urlparse(link).path
        db = os.path.basename(path)
        id = os.path.splitext(os.path.splitext(db)[0])[0]
    else:
        print("The link to CheckV database is not specified and the latest version will be automatically downloaded.")
else:
    sys.stderr.write(f"The checkv_link parameter is not present. Please, check the config file.\n")
    sys.exit(0)

# Import FASTQ files from ONT input dir
SAMPLES, EXTENSIONS = glob_wildcards(os.path.join(FASTQDIR, '{sample}_ont.{extn}'))

if SAMPLES:
    for sample in sorted(SAMPLES):
        print(f"Sample {sample} will be processed.")
else:
    sys.stderr.write(f"No files in {FASTQDIR}. Please, check the directory.\n")
    sys.exit(0)

# Check if ONT files have nonunique extension:
if EXTENSIONS:
    for extension in EXTENSIONS:
        if extension.endswith(("fastq", "fq", "fastq.gz", "fq.gz")):
            if len(set(EXTENSIONS)) != 1:
                sys.stderr.write("More than one type of file extension detected\n\t")
                sys.stderr.write("\n\t".join(set(EXTENSIONS)))
                sys.exit(0)
        else:
            sys.stderr.write("\nFile format not recognized.\n")
            sys.exit(0)
else:
    sys.stderr.write("No suitable file extension found.\n")
    sys.exit(0)

# Create sample objects:
EXTN = EXTENSIONS[0]
R1 = '{sample}_R1.' + EXTN
R2 = '{sample}_R2.' + EXTN
ONT = '{sample}_ont.' + EXTN

rule all:
  input:
    fastp_json = expand("01.pre-processing/{sample}_fastp.json", sample = SAMPLES),
    dir = expand("02.Illumina_assembly/{sample}", sample = SAMPLES),
    contigs = expand("02.Illumina_assembly/{sample}/contigs.fasta", sample = SAMPLES),
    qualimap_dir = expand("03.post-processing/mapping_evaluation/{sample}", sample = SAMPLES),
    bestscore = expand("03.post-processing/contaminants/{sample}/bestscore.blob.blobDB.table.txt", sample = SAMPLES),
    abund = expand("03.post-processing/contaminants/{sample}/{sample}_composition.txt", sample = SAMPLES),
    list = expand("03.post-processing/contaminants/{sample}/contigs.list", sample = SAMPLES),
    sel_contigs = expand("02.Illumina_assembly/{sample}/contigs_sel.fasta", sample = SAMPLES),
    quast_dir = expand("03.post-processing/assembly_evaluation/{sample}", sample = SAMPLES),
    sam = expand("03.post-processing/{sample}_map.sam", sample = SAMPLES),
    r1 = expand("03.post-processing/{sample}_sel_R1.fastq", sample = SAMPLES),
    r2 = expand("03.post-processing/{sample}_sel_R2.fastq", sample = SAMPLES),
    filt_long = expand("03.post-processing/{sample}_ont_filt.fastq", sample = SAMPLES),
    flye_dir = expand("04.ONT_assembly/{sample}", sample = SAMPLES),
    consensus_dir = expand("05.ONT_consensus/{sample}", sample = SAMPLES),
    dnaapler_dir = expand("06.fix_start/{sample}", sample = SAMPLES),
    short_cor_dir = expand("07.Illumina_correction/{sample}", sample = SAMPLES),
    sam_1 = expand("07.Illumina_correction/{sample}/align_1.sam", sample = SAMPLES),
    sam_2 = expand("07.Illumina_correction/{sample}/align_2.sam", sample = SAMPLES),
    filt_sam_1 = expand("07.Illumina_correction/{sample}/filt_align_1.sam", sample = SAMPLES),
    filt_sam_2 = expand("07.Illumina_correction/{sample}/filt_align_2.sam", sample = SAMPLES),
    polished_contigs = expand("07.Illumina_correction/{sample}/polished_contigs.fasta", sample = SAMPLES),
    fixed_contigs = expand("07.Illumina_correction/{sample}/fixed_contigs.fasta", sample = SAMPLES),
    flye_snps_dir = expand("08.SNPs/{sample}/01.flye_snps_dir", sample = SAMPLES),
    medaka_snps_dir = expand("08.SNPs/{sample}/02.medaka_snps_dir", sample = SAMPLES),
    dnaapler_snps_dir = expand("08.SNPs/{sample}/03.dnaapler_snps_dir", sample = SAMPLES),
    polypolish_snps_dir = expand("08.SNPs/{sample}/04.polypolish_snps_dir", sample = SAMPLES),
    snps_summary = expand("08.SNPs/{sample}/SNPs_summary.txt", sample = SAMPLES),
    checkm_stats = expand("03.post-processing/completeness_evaluation/{sample}/checkm_stats.tsv", sample = SAMPLES),
    checkm_lineage = expand("03.post-processing/completeness_evaluation/{sample}/lineage.ms", sample = SAMPLES),
    gtdbtk_dir = expand("09.taxonomy/{sample}", sample = SAMPLES),
    prokka_dir = expand("10.annotation/prokka/{sample}", sample = SAMPLES),
    bakta_dir = expand("10.annotation/bakta/{sample}", sample = SAMPLES),
    eggnog_dir = expand("10.annotation/eggnog/{sample}", sample = SAMPLES),
    antismash_dir = expand("10.annotation/antismash/{sample}", sample = SAMPLES),
    amr_tab = expand("11.AMR/ABRicate/{sample}/{db}.tsv", sample = SAMPLES, db = DATABASES),
    amr_summary = expand("11.AMR/ABRicate/{sample}/AMR_summary.txt", sample = SAMPLES),
    covstats = expand("11.AMR/AMR_mapping/{sample}/{sample}_covstats.tsv", sample = SAMPLES),
    amr_legend = expand("11.AMR/AMR_mapping/{sample}/{sample}_AMR_legend.tsv", sample = SAMPLES),
    plasmid_dir = expand("12.plasmids/{sample}", sample = SAMPLES),
    blast = expand("12.plasmids/{sample}/blastout", sample = SAMPLES),
    plasmids = expand("12.plasmids/{sample}/verified_plasmids.txt", sample = SAMPLES),
    vs2_dir = expand("13.phages/virsorter/{sample}", sample = SAMPLES),
    checkv_dir = expand("13.phages/checkv/{sample}", sample = SAMPLES),
    multiqc_dir = "14.report"

rule download_phix:
  output:
    phix = temp("01.pre-processing/phix.fna.gz")
  params:
    link = config['links']['phix_link']
  message:
    "--- Download PhiX genome from NCBI. ---"
  log:
    "logs/download_phix.log"
  shell:
    """
    wget {params.link} -O {output.phix} > {log} 2>&1
    """

rule build_phix:
  input:
    phix = "01.pre-processing/phix.fna.gz"
  output:
    idx1 = temp("01.pre-processing/phix.1.bt2"),
    idx2 = temp("01.pre-processing/phix.2.bt2"),
    idx3 = temp("01.pre-processing/phix.3.bt2"),
    idx4 = temp("01.pre-processing/phix.4.bt2"),
    ridx1 = temp("01.pre-processing/phix.rev.1.bt2"),
    ridx2 = temp("01.pre-processing/phix.rev.2.bt2")
  params:
    basename = "01.pre-processing/phix"
  conda:
    "envs/bowtie.yaml"
  message:
    "--- Bowtie2: Build PhiX genome db. ---"
  log:
    "logs/build_phix.log"
  shell:
    """
    bowtie2-build {input.phix} {params.basename} > {log} 2>&1
    """

rule map_phix:
  input:
    idx1 = "01.pre-processing/phix.1.bt2",
    idx2 = "01.pre-processing/phix.2.bt2",
    idx3 = "01.pre-processing/phix.3.bt2",
    idx4 = "01.pre-processing/phix.4.bt2",
    ridx1 = "01.pre-processing/phix.rev.1.bt2",
    ridx2 = "01.pre-processing/phix.rev.2.bt2",
    r1 = os.path.join(FASTQDIR, R1),
    r2 = os.path.join(FASTQDIR, R2)
  output:
    sam = temp("01.pre-processing/{sample}_contam.sam"),
    r1 = temp("01.pre-processing/{sample}.1.fastq"),
    r2 = temp("01.pre-processing/{sample}.2.fastq")
  params:
    db = temp("01.pre-processing/phix"),
    basename = "01.pre-processing/{sample}.fastq", sample = SAMPLES
  resources:
    cpus = CPUS
  conda:
    "envs/bowtie.yaml"
  message:
    "--- Bowtie2: Map short reads against PhiX genome db. ---"
  log:
    "logs/map_phix_{sample}.log"
  shell:
    """
    bowtie2 -x {params.db} -1 {input.r1} -2 {input.r2} \
    --threads {resources.cpus} --un-conc {params.basename} -S {output.sam} --local > {log} 2>&1
    """

rule trim_adapters:
  input:
    r1 = "01.pre-processing/{sample}.1.fastq",
    r2 = "01.pre-processing/{sample}.2.fastq"
  output:
    r1 = temp("01.pre-processing/{sample}_trim_R1.fastq"),
    r2 = temp("01.pre-processing/{sample}_trim_R2.fastq"),
    html = "01.pre-processing/{sample}_fastp.html",
    json = "01.pre-processing/{sample}_fastp.json"
  resources:
    cpus = 16
  conda:
    "envs/fastp.yaml"
  message:
    "--- Fastp: Remove adapters and quality filter of short reads. ---"
  log:
    "logs/trim_adapters_{sample}.log"
  shell:
    """
    fastp --detect_adapter_for_pe --length_required 100 --cut_front --cut_right --thread {resources.cpus} --verbose \
    -i {input.r1} -I {input.r2} -o {output.r1} -O {output.r2} -j {output.json} -h {output.html} > {log} 2>&1
    """

rule illumina_assembly:
  input:
    r1 = "01.pre-processing/{sample}_trim_R1.fastq",
    r2 = "01.pre-processing/{sample}_trim_R2.fastq"
  output:
    dir = directory("02.Illumina_assembly/{sample}"),
    contigs = "02.Illumina_assembly/{sample}/contigs.fasta"
  resources:
    cpus = CPUS,
    ram = RAM
  conda:
    "envs/spades.yaml"
  message:
    "--- SPAdes: genome assembly. ---"
  log:
    "logs/illumina_assembly_{sample}.log"
  priority: 0
  shell:
    """
    OMP_NUM_THREADS={resources.cpus} spades.py -k 21,33,55,77,99,127 --isolate \
    --pe1-1 {input.r1} --pe1-2 {input.r2} -o {output.dir} -t {resources.cpus} -m {resources.ram} > {log} 2>&1
    """

FASTA_LIN_CMD = r"""{if(NR==1) {printf "%s\n", $0} else {if(/^>/) {printf "\n%s\n", $0} else {printf $0}}}"""
FASTA_SEL_CMD = r"""{if(/^>/ && $6>=2.0 && $4>=500) {printf "%s\n", $0; getline; print}}"""

rule filter_contigs:
  input:
    contigs = "02.Illumina_assembly/{sample}/contigs.fasta"    
  output:
    contigs = "02.Illumina_assembly/{sample}/contigs_filt.fasta"
  message:
    "Remove short contigs <500 bp and low coverage <2x."
  priority: 0
  shell:
    """
    cat {input.contigs} | \
    awk {FASTA_LIN_CMD:q} | \
    awk -F"_" {FASTA_SEL_CMD:q} > {output.contigs}
    """

rule index_contigs:
  input:
    contigs = "02.Illumina_assembly/{sample}/contigs_filt.fasta"
  output:
    idx1 = temp("03.post-processing/{sample}_contigs.1.bt2"),
    idx2 = temp("03.post-processing/{sample}_contigs.2.bt2"),
    idx3 = temp("03.post-processing/{sample}_contigs.3.bt2"),
    idx4 = temp("03.post-processing/{sample}_contigs.4.bt2"),
    ridx1 = temp("03.post-processing/{sample}_contigs.rev.1.bt2"),
    ridx2 = temp("03.post-processing/{sample}_contigs.rev.2.bt2")
  params:
    basename = "03.post-processing/{sample}_contigs"
  conda:
    "envs/bowtie.yaml"
  message:
    "--- Bowtie2: Build Illumina contig db. ---"
  log:
    "logs/index_contigs_{sample}.log"
  priority: 0
  shell:
    """
    bowtie2-build -f {input.contigs} {params.basename} > {log} 2>&1
    """

rule map_contigs:
  input:
    idx1 = "03.post-processing/{sample}_contigs.1.bt2",
    idx2 = "03.post-processing/{sample}_contigs.2.bt2",
    idx3 = "03.post-processing/{sample}_contigs.3.bt2",
    idx4 = "03.post-processing/{sample}_contigs.4.bt2",
    ridx1 = "03.post-processing/{sample}_contigs.rev.1.bt2",
    ridx2 = "03.post-processing/{sample}_contigs.rev.2.bt2",
    r1 = "01.pre-processing/{sample}_trim_R1.fastq",
    r2 = "01.pre-processing/{sample}_trim_R2.fastq"
  output:
    bam = temp("03.post-processing/{sample}_map.bam"),
    bai = temp("03.post-processing/{sample}_map.bam.bai"),
    csi = temp("03.post-processing/{sample}_map.bam.csi")
  params:
    db = temp("03.post-processing/{sample}_contigs")
  resources:
    cpus = CPUS
  conda:
    "envs/bowtie.yaml"
  message:
    "--- Bowtie2: Map reads against contigs. ---"
  log:
    "logs/map_contigs_{sample}.log"
  priority: 0
  shell:
    """
    bowtie2 -x {params.db} -1 {input.r1} -2 {input.r2} -p {resources.cpus} -t 2> {log} | \
    samtools view -@ {resources.cpus} -hbS - | \
    samtools sort -@ {resources.cpus} --write-index -o {output.bam} - >> {log} 2>&1
    samtools index -b {output.bam} -@ {resources.cpus} >> {log} 2>&1
    """

rule map_evaluation:
  input:
    bam = "03.post-processing/{sample}_map.bam"
  output:
    qualimap_dir = directory("03.post-processing/mapping_evaluation/{sample}")
  conda:
    "envs/qualimap.yaml"
  message:
    "--- Qualimap: Mapping evaluation. ---"
  log:
    "logs/map_evaluation_{sample}.log"
  shell:
    """
    qualimap bamqc -bam {input.bam} -outdir {output.qualimap_dir} -outformat html > {log} 2>&1
    """

rule blast_contigs:
  input:
    contigs = "02.Illumina_assembly/{sample}/contigs_filt.fasta"
  output:
    blast = "03.post-processing/contaminants/{sample}/blastout"
  params:
    dir = BLASTDB,
    db = os.path.join(BLASTDB, "nt")
  resources:
    cpus = CPUS
  conda:
    "envs/blast.yaml"
  message:
    "--- BLAST: Contigs against NCBI nt db. ---"
  log:
    "logs/blast_contigs_{sample}.log"
  priority: 0
  shell:
    """
    BLASTDB={params.dir} blastn -task megablast -query {input.contigs} -db {params.db} -outfmt \
    '6 qseqid staxids bitscore pident evalue length qlen slen qcovs qcovhsp sskingdoms scomnames sscinames sblastnames stitle' \
    -num_threads {resources.cpus} -evalue 1e-5 -max_target_seqs 100 -max_hsps 10 \
    -out {output.blast} > {log} 2>&1
    """

rule blob_json:
  input:
    contigs = "02.Illumina_assembly/{sample}/contigs_filt.fasta",
    bam = "03.post-processing/{sample}_map.bam",
    bai = "03.post-processing/{sample}_map.bam.bai",
    blast = "03.post-processing/contaminants/{sample}/blastout",
    nodes = os.path.join(BLASTDB, "nodes.dmp"),
    names = os.path.join(BLASTDB, "names.dmp")
  output:
    json = temp("03.post-processing/contaminants/{sample}/blob.blobDB.json"),
    cov = temp("03.post-processing/contaminants/{sample}/blob.{sample}_map.bam.cov")
  params:
    basename = "03.post-processing/contaminants/{sample}/blob"
  conda:
    "envs/blobtools.yaml"
  message:
    "--- BlobTools: Screen BLAST hits for contaminants. ---"
  log:
    "logs/blob_json_{sample}.log"
  priority: 0
  shell:
    """
    blobtools create -i {input.contigs} -b {input.bam} -t {input.blast} --nodes {input.nodes} --names {input.names} \
    -o {params.basename} > {log} 2>&1
    """

rule blob_table:
  input:
    json = "03.post-processing/contaminants/{sample}/blob.blobDB.json"
  output:
    bestscore = "03.post-processing/contaminants/{sample}/bestscore.blob.blobDB.table.txt"
  params:
    basename = "03.post-processing/contaminants/{sample}/bestscore"
  conda:
    "envs/blobtools.yaml"
  message:
    "--- BlobTools: Collapse taxonomic assignment of BLAST hits according to sum of best scores. ---"
  log:
    "logs/blob_table_{sample}.log"
  priority: 0
  shell:
    """
    blobtools view --input {input.json} --out {params.basename} --taxrule bestsum --rank all --hits >> {log} 2>&1
    """

# Execute either one rule or another according to presence/absence of 'genus' parameter
if config.get('parameters') is not None and "genus" in config['parameters'] and config['parameters']['genus'] is not None and len(config['parameters']['genus']) > 0:
  rule:
    input:
      bestscore = "03.post-processing/contaminants/{sample}/bestscore.blob.blobDB.table.txt",
      contigs = "02.Illumina_assembly/{sample}/contigs_filt.fasta"
    output:
      abund = "03.post-processing/contaminants/{sample}/{sample}_composition.txt",
      list = "03.post-processing/contaminants/{sample}/contigs.list",
      contigs = "02.Illumina_assembly/{sample}/contigs_sel.fasta"
    params:
      genus = config['parameters']['genus']    
    priority: 0
    shell:
      """
      for i in $(cat {input.bestscore} | sed '1,11d' | cut -f 22 | sort -u); do \
      cat {input.bestscore} | sed '1,11d' | awk -v var=$i 'BEGIN {{printf "%s%s", var, ": "}} $22 == var {{count++}} END {{printf "%.2f\\n", count/NR}}'; \
      done > {output.abund}
      echo "Sample {wildcards.sample} composition:"
      cat {output.abund}
      awk -v var="{params.genus}" 'tolower($22) ~ tolower("[:alpha:]*"var) {{print $1}}' {input.bestscore} > {output.list}
      grep -A1 -f {output.list} {input.contigs} | sed '/--/d' > {output.contigs}
      """
elif config.get('parameters') is not None and "genus" in config['parameters'] and (config['parameters']['genus'] is None or len(config['parameters']['genus']) == 0):
  rule:
    input:
      bestscore = "03.post-processing/contaminants/{sample}/bestscore.blob.blobDB.table.txt",
      contigs = "02.Illumina_assembly/{sample}/contigs_filt.fasta"
    output:
      abund = "03.post-processing/contaminants/{sample}/{sample}_composition.txt",
      list = "03.post-processing/contaminants/{sample}/contigs.list",
      contigs = "02.Illumina_assembly/{sample}/contigs_sel.fasta"
    priority: 0
    shell:
      """
      for i in $(cat {input.bestscore} | sed '1,11d' | cut -f 22 | sort -u); do \
      cat {input.bestscore} | sed '1,11d' | awk -v var=$i 'BEGIN {{printf "%s%s", var, ": "}} $22 == var {{count++}} END {{printf "%.2f\\n", count/NR}}'; \
      done > {output.abund}
      echo "Sample {wildcards.sample} composition:"
      cat {output.abund}
      for i in $(cat {output.abund} | sort -t':' -k2 -nr | cut -d':' -f1 | sed -n '1p' | sed -e 's/Para//;s/Pseudo//;s/Paen//;s/Paeni//;s/Brady//;s/Meso//;s/Neo//;s/Sino//;s/Aeri//;s/Caldi//;s/Geo//' | tr '[:upper:]' '[:lower:]'); do \
      awk -v var="$i" 'tolower($22) ~ tolower("[:alpha:]*"var) {{print $1}}' {input.bestscore}; \
      done > {output.list}
      grep -A1 -f {output.list} {input.contigs} | sed '/--/d' > {output.contigs}
      """
else:
  rule:
    input:
      bestscore = "03.post-processing/contaminants/{sample}/bestscore.blob.blobDB.table.txt",
      contigs = "02.Illumina_assembly/{sample}/contigs_filt.fasta"
    output:
      abund = "03.post-processing/contaminants/{sample}/{sample}_composition.txt",
      list = "03.post-processing/contaminants/{sample}/contigs.list",
      contigs = "02.Illumina_assembly/{sample}/contigs_sel.fasta"
    params:
      nohit = "no-hit"   
    priority: 0
    shell:
      """
      for i in $(cat {input.bestscore} | sed '1,11d' | cut -f 22 | sort -u); do \
      cat {input.bestscore} | sed '1,11d' | awk -v var=$i 'BEGIN {{printf "%s%s", var, ": "}} $22 == var {{count++}} END {{printf "%.2f\\n", count/NR}}'; \
      done > {output.abund}
      echo "Sample {wildcards.sample} composition:"
      cat {output.abund}
      awk -v var="{params.nohit}" 'tolower($22) !~ tolower("[:alpha:]*"var) {{print $1}}' {input.bestscore} > {output.list}
      grep -A1 -f {output.list} {input.contigs} | sed '/--/d' > {output.contigs}
      """

rule genome_assembly_evaluation:
  input:
    contigs = "02.Illumina_assembly/{sample}/contigs_sel.fasta"
  output:
    quast_dir = directory("03.post-processing/assembly_evaluation/{sample}")
  resources:
    cpus = CPUS
  conda:
    "envs/quast.yaml"
  message:
    "--- QUAST: Genome assembly evaluation. ---"
  log:
    "logs/assembly_evaluation_{sample}.log"
  shell:
    """
    quast {input.contigs} -o {output.quast_dir} --no-icarus -t {resources.cpus} > {log} 2>&1 
    """

rule index_selected_contigs:
  input:
    contigs = "02.Illumina_assembly/{sample}/contigs_sel.fasta"
  output:
    idx1 = temp("03.post-processing/{sample}_contigs_sel.1.bt2"),
    idx2 = temp("03.post-processing/{sample}_contigs_sel.2.bt2"),
    idx3 = temp("03.post-processing/{sample}_contigs_sel.3.bt2"),
    idx4 = temp("03.post-processing/{sample}_contigs_sel.4.bt2"),
    ridx1 = temp("03.post-processing/{sample}_contigs_sel.rev.1.bt2"),
    ridx2 = temp("03.post-processing/{sample}_contigs_sel.rev.2.bt2")
  params:
    basename = "03.post-processing/{sample}_contigs_sel"
  conda:
    "envs/bowtie.yaml"
  message:
    "--- Bowtie2: Build selected contig db. ---"
  log:
    "logs/index_selected_contigs_{sample}.log"
  priority: 0
  shell:
    """
    bowtie2-build -f {input.contigs} {params.basename} > {log} 2>&1
    """

rule map_sel_contigs:
  input:
    idx1 = "03.post-processing/{sample}_contigs_sel.1.bt2",
    idx2 = "03.post-processing/{sample}_contigs_sel.2.bt2",
    idx3 = "03.post-processing/{sample}_contigs_sel.3.bt2",
    idx4 = "03.post-processing/{sample}_contigs_sel.4.bt2",
    ridx1 = "03.post-processing/{sample}_contigs_sel.rev.1.bt2",
    ridx2 = "03.post-processing/{sample}_contigs_sel.rev.2.bt2",
    r1 = "01.pre-processing/{sample}_trim_R1.fastq",
    r2 = "01.pre-processing/{sample}_trim_R2.fastq"
  output:
    sam = temp("03.post-processing/{sample}_map.sam"),
    r1 = temp("03.post-processing/{sample}_sel_R1.fastq"),
    r2 = temp("03.post-processing/{sample}_sel_R2.fastq")
  params:
    db = temp("03.post-processing/{sample}_contigs_sel")
  resources:
    cpus = CPUS
  conda:
    "envs/bowtie.yaml"
  message:
    "--- Bowtie2: Map trimmed reads against selected contigs. ---"
  log:
    "logs/map_selected_contigs_{sample}.log"
  priority: 0
  shell:
    """
    bowtie2 -x {params.db} -1 {input.r1} -2 {input.r2} -p {resources.cpus} -t --no-unal -S {output.sam} > {log} 2>&1

    samtools fastq -f 0x2 -1 {output.r1} -2 {output.r2} {output.sam} >> {log} 2>&1
    """

for sample in SAMPLES:
  rule:
    input:
      long = expand(os.path.join(FASTQDIR, '{sample}_ont.' + EXTN), sample = sample),
      r1 = expand("03.post-processing/{sample}_sel_R1.fastq", sample = sample),
      r2 = expand("03.post-processing/{sample}_sel_R2.fastq", sample = sample),
    output:
      filt_long = temp(expand("03.post-processing/{sample}_ont_filt.fastq", sample = sample))
    params:
      min_length = 1000,
      split = 1000,
      keep_percent = 90,
      length_weight = 10
    conda:
      "envs/filtlong.yaml"
    message:
      "--- Filtlong: Filter long reads with short read correction. ---"
    log:
      expand("logs/filter_long_{sample}.log", sample = sample)
    shell:
      """
      filtlong -1 {input.r1} -2 {input.r2} \
      --min_length {params.min_length} --trim --split {params.split} --keep_percent {params.keep_percent} --length_weight {params.length_weight} {input.long} \
      > {output.filt_long} 2>{log}
      """

rule ONT_assembly:
  input:
    filt_long = "03.post-processing/{sample}_ont_filt.fastq"
  output:
    flye_dir = directory("04.ONT_assembly/{sample}")
  params:
    iterations = 5
  resources:
    cpus = CPUS
  conda:
    "envs/flye.yaml"
  message:
    "--- Flye: Genome assembly with long reads. ---"
  log:
    "logs/genome_assembly_{sample}.log"
  shell:
    """
    flye --nano-hq {input.filt_long} --out-dir {output.flye_dir} \
    --threads {resources.cpus} --iterations {params.iterations} > {log} 2>&1
    """

# Conditional rule execution according to presence/absence of 'medaka_model' parameter
if "medaka_model" in config['parameters'] and config['parameters']['medaka_model'] is not None and len(config['parameters']['medaka_model']) > 0:
  rule:
    input:
      filt_long = "03.post-processing/{sample}_ont_filt.fastq",
      flye_dir = "04.ONT_assembly/{sample}"
    output:
      consensus_dir = directory("05.ONT_consensus/{sample}")
    params:
      model = config['parameters']['medaka_model'] 
    resources:
      cpus = 2
    conda:
      "envs/medaka.yaml"
    message:
      "--- Medaka: Improve contig consensus with long reads. ---"
    log:
      "logs/consensus_long_{sample}.log"
    shell:
      """
      medaka_consensus -i {input.filt_long} -d {input.flye_dir}/assembly.fasta -t {resources.cpus} -m {params.model} \
      -o {output.consensus_dir} > {log} 2>&1
      """
else:
  rule:
    input:
      filt_long = "03.post-processing/{sample}_ont_filt.fastq",
      flye_dir = "04.ONT_assembly/{sample}"
    output:
      consensus_dir = directory("05.ONT_consensus/{sample}") 
    resources:
      cpus = 2
    conda:
      "envs/medaka.yaml"
    message:
      "--- Medaka: Improve contig consensus with long reads. ---"
    log:
      "logs/consensus_long_{sample}.log"
    shell:
      """
      for i in $(medaka tools list_models | grep -i "default consensus" | cut -d" " -f4); do \
          medaka_consensus -i {input.filt_long} -d {input.flye_dir}/assembly.fasta -t {resources.cpus} -m $i -o {output.consensus_dir}
      done > {log} 2>&1
      """

rule fix_start:
  input:
    consensus_dir = "05.ONT_consensus/{sample}",
  output:
    dnaapler_dir = directory("06.fix_start/{sample}")
  params:
    prefix = "assembly",
    evalue = 1e-10
  resources:
    cpus = CPUS
  conda:
    "envs/dnaapler.yaml"
  message:
    "--- dnaapler: Re-orient replicons. ---"
  log:
    "logs/fix_start_{sample}.log"
  shell:
    """
    dnaapler all -i {input.consensus_dir}/consensus.fasta -p {params.prefix} -e {params.evalue} -t {resources.cpus} \
    -o {output.dnaapler_dir} >> {log} 2>&1
    """

for sample in SAMPLES:
  rule:
    input:
      dnaapler_dir = expand("06.fix_start/{sample}", sample = sample),
      r1 = expand("03.post-processing/{sample}_sel_R1.fastq", sample = sample),
      r2 = expand("03.post-processing/{sample}_sel_R2.fastq", sample = sample),
    output:
      short_cor_dir = directory(expand("07.Illumina_correction/{sample}", sample = sample)),
      sam_1 = temp(expand("07.Illumina_correction/{sample}/align_1.sam", sample = sample)),
      sam_2 = temp(expand("07.Illumina_correction/{sample}/align_2.sam", sample = sample)),
      filt_sam_1 = temp(expand("07.Illumina_correction/{sample}/filt_align_1.sam", sample = sample)),
      filt_sam_2 = temp(expand("07.Illumina_correction/{sample}/filt_align_2.sam", sample = sample)),
      polished_contigs = temp(expand("07.Illumina_correction/{sample}/polished_contigs.fasta", sample = sample)),
      fixed_contigs = expand("07.Illumina_correction/{sample}/fixed_contigs.fasta", sample = sample)
    resources:
      cpus = CPUS
    conda:
      "envs/polypolish.yaml"
    message:
      "--- Polypolish: Correct contigs with short reads. ---"
    log:
      expand("logs/correction_short_{sample}.log", sample = sample)
    shell:
      """
      #align short reads to reoriented contigs
      bwa index {input.dnaapler_dir}/assembly_reoriented.fasta > {log} 2>&1
      bwa mem -t {resources.cpus} -a {input.dnaapler_dir}/assembly_reoriented.fasta {input.r1} 2>> {log} > {output.sam_1}
      bwa mem -t {resources.cpus} -a {input.dnaapler_dir}/assembly_reoriented.fasta {input.r2} 2>> {log} > {output.sam_2}

      #filter the alignment
      polypolish filter --in1 {output.sam_1} --in2 {output.sam_2} --out1 {output.filt_sam_1} --out2 {output.filt_sam_2} >> {log} 2>&1
      
      #error correction
      polypolish polish {input.dnaapler_dir}/assembly_reoriented.fasta {output.filt_sam_1} {output.filt_sam_2} 2>> {log} > {output.polished_contigs}

      # fix headers
      awk -F' ' '/^>/{{print $1; next}}{{print}}' {output.polished_contigs} 2>> {log} > {output.fixed_contigs}

      # clean directory
      rm -rf {output.short_cor_dir}/*.amb \
      {output.short_cor_dir}/*.ann \
      {output.short_cor_dir}/*.bwt \
      {output.short_cor_dir}/*.pac \
      {output.short_cor_dir}/*.sa
      """

for sample in SAMPLES:
  rule:
    input:
      sel_contigs = expand("02.Illumina_assembly/{sample}/contigs_sel.fasta", sample = sample),
      flye_dir = expand("04.ONT_assembly/{sample}", sample = sample),
      consensus_dir = expand("05.ONT_consensus/{sample}", sample = sample),
      dnaapler_dir = expand("06.fix_start/{sample}", sample = sample),
      fixed_contigs = expand("07.Illumina_correction/{sample}/fixed_contigs.fasta", sample = sample)
    output:
      flye_snps_dir = directory(expand("08.SNPs/{sample}/01.flye_snps_dir", sample = sample)),
      medaka_snps_dir = directory(expand("08.SNPs/{sample}/02.medaka_snps_dir", sample = sample)),
      dnaapler_snps_dir = directory(expand("08.SNPs/{sample}/03.dnaapler_snps_dir", sample = sample)),
      polypolish_snps_dir = directory(expand("08.SNPs/{sample}/04.polypolish_snps_dir", sample = sample)),
      snps_summary = expand("08.SNPs/{sample}/SNPs_summary.txt", sample = sample)
    resources:
      cpus = CPUS
    conda:
      "envs/snippy.yaml"
    message:
      "--- Snippy: Differences between long-read assemblies and a short-read assembly as reference. ---"
    log:
      expand("logs/snps_{sample}.log", sample = sample)
    shell:
      """
      # flye assembly SNPs
      snippy --prefix "01.long-read_assembly" --ref {input.sel_contigs} --ctgs {input.flye_dir}/assembly.fasta --cpus {resources.cpus} \
      --outdir {output.flye_snps_dir} > {log} 2>&1

      # medaka correction SNPs
      snippy --prefix "02.long-read_correction" --ref {input.sel_contigs} --ctgs {input.consensus_dir}/consensus.fasta --cpus {resources.cpus} \
      --outdir {output.medaka_snps_dir} >> {log} 2>&1

      # dnaapler reorientation SNPs
      snippy --prefix "03.replicon_reorientation" --ref {input.sel_contigs} --ctgs {input.dnaapler_dir}/assembly_reoriented.fasta --cpus {resources.cpus} \
      --outdir {output.dnaapler_snps_dir} >> {log} 2>&1

      # polypolish correction SNPs
      snippy --prefix "04.short-read_correction" --ref {input.sel_contigs} --ctgs {input.fixed_contigs} --cpus {resources.cpus} \
      --outdir {output.polypolish_snps_dir} >> {log} 2>&1

      # SNPs summary of differences between each long-read assembly vs short-read assembled selected contigs
      echo "01. Long-read assembly" > {output.snps_summary}
      grep "Variant" {output.flye_snps_dir}/*.txt >> {output.snps_summary}  
      echo -e "\n02. Long-read correction" >> {output.snps_summary}
      grep "Variant" {output.medaka_snps_dir}/*.txt >> {output.snps_summary}
      echo -e "\n03. Replicon reorientation" >> {output.snps_summary}
      grep "Variant" {output.dnaapler_snps_dir}/*.txt >> {output.snps_summary}
      echo -e "\n04. Short-read correction" >> {output.snps_summary}
      grep "Variant" {output.polypolish_snps_dir}/*.txt >> {output.snps_summary}
      echo -e "\nShort-read assembled selected contigs were used as reference." >> {output.snps_summary}
      """

rule completeness_and_contamination:
  input:
    illumina_contigs = "02.Illumina_assembly/{sample}/contigs_sel.fasta",
    ont_contigs = "07.Illumina_correction/{sample}/fixed_contigs.fasta"
  output:
    checkm_dir = directory("03.post-processing/completeness_evaluation/{sample}"),
    checkm_stats = "03.post-processing/completeness_evaluation/{sample}/checkm_stats.tsv",
    checkm_lineage = "03.post-processing/completeness_evaluation/{sample}/lineage.ms"
  resources:
    cpus = CPUS
  conda:
    "envs/checkm.yaml"
  message:
    "--- CheckM: Assessment of genome completenness and contamination. ---"
  log:
    "logs/completenness_and_contamination_{sample}.log"
  priority: 0
  shell:
    """
    cp {input.illumina_contigs} {input.ont_contigs} {output.checkm_dir}
    checkm lineage_wf -t {resources.cpus} -x fasta {output.checkm_dir} {output.checkm_dir} > {log} 2>&1
    checkm qa -o 2 -t {resources.cpus} --tab_table -f {output.checkm_stats} {output.checkm_lineage} {output.checkm_dir} >> {log} 2>&1
    """

rule taxonomic_assignment:
  input:
    checkm_dir = "03.post-processing/completeness_evaluation/{sample}",
  output:
    gtdbtk_dir = directory("09.taxonomy/{sample}")
  params:
    gtdbtk_db = GTDBTKDB
  resources:
    cpus = CPUS,
    cpus_p = min(CPUS, 64)
  conda:
    "envs/gtdbtk.yaml"
  message:
    "--- GTDB-Tk: Taxonomic assignment. ---"
  log:
    "logs/taxonomic_assignment_{sample}.log"
  priority: 0
  shell:
    """
    GTDBTK_DATA_PATH={params.gtdbtk_db:q} \
    gtdbtk classify_wf -x fasta --genome_dir {input.checkm_dir} --cpus {resources.cpus} --pplacer_cpus {resources.cpus_p} --mash_db {params.gtdbtk_db:q}/mash/gtdb-tk_r214.msh \
    --out_dir {output.gtdbtk_dir} > {log} 2>&1

    rm -rf {input.checkm_dir}/contigs*.fasta
    """

rule legacy_annotation:
  input:
    ont_contigs = "07.Illumina_correction/{sample}/fixed_contigs.fasta",
    abund = "03.post-processing/contaminants/{sample}/{sample}_composition.txt"
  output:
    prokka_dir = directory("10.annotation/prokka/{sample}")
  resources:
    cpus = CPUS
  conda:
    "envs/prokka.yaml"
  message:
    "--- PROKKA: Genome annotation. ---"
  log:
    "logs/annotation_{sample}.log"
  priority: 0
  shell:
    """
    for i in $(cat {input.abund} | sort -t':' -k2 -nr | cut -d':' -f1 | sed -n '1p'); do \
        prokka --kingdom Bacteria --genus $i --species sp. --strain {wildcards.sample} \
        --usegenus --gcode 11 --rfam --compliant --addgenes --mincontiglen 500 \
        --centre AIT --locustag {wildcards.sample} --prefix {wildcards.sample} \
        --outdir {output.prokka_dir} --cpus {resources.cpus} --force {input.ont_contigs}; \
    done > {log} 2>&1
    """

rule accurate_annotation:
  input:
    ont_contigs = "07.Illumina_correction/{sample}/fixed_contigs.fasta",
    abund = "03.post-processing/contaminants/{sample}/{sample}_composition.txt"
  output:
    bakta_dir = directory("10.annotation/bakta/{sample}")
  params:
    bakta_db = BAKTADB
  resources:
    cpus = CPUS
  conda:
    "envs/bakta.yaml"
  message:
    "--- Bakta: Genome annotation. ---"
  log:
    "logs/better_annotation_{sample}.log"
  priority: 0
  shell:
    """
    for i in $(cat {input.abund} | sort -t':' -k2 -nr | cut -d':' -f1 | sed -n '1p'); do \
        bakta --db {params.bakta_db} --verbose --genus $i --species sp. --strain {wildcards.sample} \
        --translation-table 11 --min-contig-length 500 \
        --locus-tag {wildcards.sample} --prefix {wildcards.sample} \
        --output {output.bakta_dir} --threads {resources.cpus} --force {input.ont_contigs}; \
    done > {log} 2>&1
    """    

rule functional_annotation:
  input:
    bakta_dir = "10.annotation/bakta/{sample}"
  output:
    temp_dir = temp(directory("10.annotation/eggnog/{sample}/eggnog_tmp")),
    eggnog_dir = directory("10.annotation/eggnog/{sample}")
  params:
    dmnd_db = DMNDDB
  resources:
    cpus = CPUS
  conda:
    "envs/eggnog-mapper.yaml"
  message:
    "--- EggNOG: Functional annotation. ---"
  log:
    "logs/functional_annotation_{sample}.log"
  priority: 0
  shell:
    """
    mkdir -p {output.temp_dir} {output.eggnog_dir}

    emapper.py -i {input.bakta_dir}/{wildcards.sample}.faa --output_dir {output.eggnog_dir} \
    --cpu {resources.cpus} -m diamond --data_dir {params.dmnd_db} \
    --output {wildcards.sample} --temp_dir {output.temp_dir} --override > {log} 2>&1
    """

rule secondary_metabolites_db:
  output:
    antismash_db = temp(directory("10.annotation/antismash/databases"))
  conda:
    "envs/antismash.yaml"
  message:
    "--- antiSMASH: database download. ---"
  log:
    "logs/secondary_metabolites_database.log"
  priority: 4
  shell:
    """
    download-antismash-databases --database-dir {output.antismash_db} > {log} 2>&1
    """    

rule secondary_metabolites_analysis:
  input:
    antismash_db = "10.annotation/antismash/databases",
    bakta_dir = "10.annotation/bakta/{sample}"
  output:
    antismash_dir = directory("10.annotation/antismash/{sample}")
  params:
    taxon = 'bacteria',
    genefinding_tool = 'none'
  conda:
    "envs/antismash.yaml"
  message:
    "--- antiSMASH: secondary metabolite annotation. ---"
  log:
    "logs/secondary_metabolites_{sample}.log"
  priority: 4
  shell:
    """
    antismash --output-dir {output.antismash_dir} --output-basename {wildcards.sample} \
    --databases {input.antismash_db} --taxon {params.taxon} --genefinding-tool {params.genefinding_tool} \
    {input.bakta_dir}/{wildcards.sample}.gbff > {log} 2>&1
    """ 

for sample in SAMPLES:
  for db in DATABASES:
    rule:
      input:
        contigs = expand("02.Illumina_assembly/{sample}/contigs_sel.fasta", sample = sample)
      output:
        amr_tab = expand("11.AMR/ABRicate/{sample}/{db}.tsv", sample = sample, db = db)
      params:
        db = db
      conda:
        "envs/abricate.yaml"
      message:
        "--- ABRicate: AMR detection. ---"
      log:
        expand("logs/amr_{db}_in_{sample}_contigs.log", sample = sample, db = db)
      shell:
        """
        abricate --db {params.db} {input.contigs} --nopath --quiet > {output.amr_tab}
        """
 
rule AMR_summary:
  input:
    argannot = "11.AMR/ABRicate/{sample}/argannot.tsv",
    card = "11.AMR/ABRicate/{sample}/card.tsv",
    ecoh = "11.AMR/ABRicate/{sample}/ecoh.tsv",
    ecoli_vf = "11.AMR/ABRicate/{sample}/ecoli_vf.tsv",
    megares = "11.AMR/ABRicate/{sample}/megares.tsv",
    ncbi = "11.AMR/ABRicate/{sample}/ncbi.tsv",
    resfinder = "11.AMR/ABRicate/{sample}/resfinder.tsv",
    vfdb = "11.AMR/ABRicate/{sample}/vfdb.tsv"
  output:
    amr_summary = "11.AMR/ABRicate/{sample}/AMR_summary.txt"
  conda:
    "envs/abricate.yaml"
  shell:
    """
    abricate --summary \
    {input.argannot} {input.card} {input.ecoh} \
    {input.ecoli_vf} {input.megares} {input.ncbi} \
    {input.resfinder} {input.vfdb} > {output.amr_summary}
    """

rule download_amr_db:
  output:
    card_db = temp("11.AMR/AMR_db/card.tar.bz2"),
    card_dir = temp(directory("11.AMR/AMR_db"))
  params:
    link = config['links']['card_link'],
  message:
    "--- Download AMR features from CARD repository. ---"
  log:
    "logs/download_amr.log"
  priority: 10
  shell:
    """
    wget {params.link} -O {output.card_db} > {log} 2>&1
    tar -xjvf {output.card_db} -C {output.card_dir} >> {log} 2>&1
    """

rule map_amr_db:
  input:
    card_dir = "11.AMR/AMR_db",
    r1 = "01.pre-processing/{sample}_trim_R1.fastq",
    r2 = "01.pre-processing/{sample}_trim_R2.fastq"
  output:
    bbmap_temp = temp(directory("11.AMR/AMR_mapping/{sample}/ref")),  
    covstats_temp = temp("11.AMR/AMR_mapping/{sample}/{sample}_covstats_temp.tsv"),
    covstats = "11.AMR/AMR_mapping/{sample}/{sample}_covstats.tsv",
    amr_legend = "11.AMR/AMR_mapping/{sample}/{sample}_AMR_legend.tsv"
  params:
    card_target = "nucleotide_fasta_protein_homolog_model.fasta",
    min_id = 0.99
  resources:
    cpus = CPUS
  conda:
    "envs/bbmap.yaml"
  log:
    "logs/map_amr_{sample}.log"  
  priority: 9
  message:
    "--- Map trimmed reads against CARD db. ---"
  shell:
    """
    bbmap.sh -in={input.r1} -in2={input.r2} ref={input.card_dir}/{params.card_target} path={output.bbmap_temp} \
    idfilter={params.min_id} idtag -Xmx64g threads={resources.cpus} ambiguous=best secondary=f \
    covstats={output.covstats_temp} > {log} 2>&1

    (head -n 1 {output.covstats_temp} > {output.covstats}) && \
    tail -n +2 {output.covstats_temp} | awk -F'\t' '{{print $5 "\t" $0}}' | sort -t$'\t' -k1,1nr | cut -f2- >> \
    {output.covstats}

    echo "#AMR features with a covered length of at least 70%" > {output.amr_legend}
    (head -n 1 {input.card_dir}/aro_index.tsv >> {output.amr_legend}) && \
    for i in $(tail -n +2 {output.covstats} | awk -F'\t' '$5 >=70' | cut -f1 | awk -F'|' '{{print $5}}'); do \
        grep $i {input.card_dir}/aro_index.tsv
    done >> {output.amr_legend}
    """

rule plasmid_search:
  input:
    ont_contigs = "07.Illumina_correction/{sample}/fixed_contigs.fasta"
  output:
    plasmid_dir = directory("12.plasmids/{sample}"),
    blast = "12.plasmids/{sample}/blastout",
    plasmids = "12.plasmids/{sample}/verified_plasmids.txt"
  params:
    platon_db = PLATONDB,
    dir = BLASTDB,
    db = os.path.join(BLASTDB, "nt")
  resources:
    cpus = CPUS
  conda:
    "envs/platon.yaml"
  message:
    "--- Platon: Plasmid identification. ---"
  log:
    "logs/plasmid_search_{sample}.log"
  priority: 0
  shell:
    """
    platon --db {params.platon_db} --output {output.plasmid_dir} --verbose --threads {resources.cpus} {input.ont_contigs} > {log} 2>&1
    
    BLASTDB={params.dir} blastn -task megablast -query {input.ont_contigs} -db {params.db} -outfmt \
    '6 qseqid staxids bitscore pident evalue length qlen slen qcovs qcovhsp sskingdoms scomnames sscinames sblastnames stitle' \
    -num_threads {resources.cpus} -evalue 1e-5 -max_target_seqs 100 -max_hsps 10 \
    -out {output.blast} >> {log} 2>&1

    if [[ -s {output.plasmid_dir}/fixed_contigs.plasmid.fasta ]] && grep -q ">" {output.plasmid_dir}/fixed_contigs.plasmid.fasta; then
      while IFS= read -r i; do
        if grep -m 1 "$i" {output.blast} | grep -q "plasmid"; then
            echo "{wildcards.sample}: $i is a plasmid." >> {output.plasmids}
        else
            echo "{wildcards.sample}: $i was not verified by BLAST search." >> {output.plasmids}
        fi
      done < <(grep ">" {output.plasmid_dir}/fixed_contigs.plasmid.fasta | sed 's/^>//g')
    else
      if [[ -v i ]]; then
        echo "Platon found no plasmid in {wildcards.sample} $i." > {output.plasmids}
      else
        echo "Platon found no plasmid in {wildcards.sample}." > {output.plasmids}
      fi
    fi
    """

rule viral_db:
  output:
    vs2_db = temp(directory("13.phages/vs2_db")),
    checkv_db = temp(directory("13.phages/checkv_db"))
  resources:
    cpus = 4
  conda:
    "envs/virsorter.yaml"
  params:
    checkv_link = config['links']['checkv_link'] if 'checkv_link' in config['links'] else None,
    tries = 5,
    db_id = id
  message:
    """
    --- Download VirSort2 database ---
    --- Download CheckV database ---
    """
  log:
    "logs/viral_databases.log"
  priority: 9
  shell:
    """
    virsorter setup -d {output.vs2_db} -j {resources.cpus} > {log} 2>&1
    if [ -z "{params.checkv_link}" ]; then
        checkv download_database {output.checkv_db} >> {log} 2>&1
    else
        wget --tries={params.tries} -c {params.checkv_link} -P {output.checkv_db} >> {log} 2>&1
        tar -xzvf {output.checkv_db}/{params.db_id}.tar.gz -C {output.checkv_db} >> {log} 2>&1
        diamond makedb --in {output.checkv_db}/{params.db_id}/genome_db/checkv_reps.faa \
        --db {output.checkv_db}/{params.db_id}/genome_db/checkv_reps >> {log} 2>&1
    fi
    """

rule viral_identification:
  input:
    contigs = "02.Illumina_assembly/{sample}/contigs_filt.fasta",
    vs2_db = "13.phages/vs2_db",
    checkv_db = "13.phages/checkv_db"
  output:
    vs2_dir = directory("13.phages/virsorter/{sample}"),
    checkv_dir = directory("13.phages/checkv/{sample}")
  params:
    viral_groups = "dsDNAphage,NCLDV,RNA,ssDNA,lavidaviridae",
    min_score = 0.5,
    checkv_link = config['links']['checkv_link'] if 'checkv_link' in config['links'] else None,
    db_id = id
  resources:
    cpus = CPUS
  conda:
    "envs/virsorter.yaml"
  message:
    """
    --- VirSorter2: Identification of phages and prophages. ---
    --- CheckV: Quality assessment of viral genomes. ---
    """
  log:
    "logs/viral_identification_{sample}.log"
  priority: 8
  shell:
    """
    virsorter run  -i {input.contigs} -w {output.vs2_dir} -d {input.vs2_db} \
    --keep-original-seq --include-groups {params.viral_groups} --min-score {params.min_score} -j {resources.cpus} all > {log} 2>&1

    if [ -z "{params.checkv_link}" ]; then
        checkv end_to_end {output.vs2_dir}/final-viral-combined.fa {output.checkv_dir} \
      -t {resources.cpus} -d {input.checkv_db}/{params.db_id} >> {log} 2>&1 
    else
        checkv end_to_end {output.vs2_dir}/final-viral-combined.fa {output.checkv_dir} \
      -t {resources.cpus} -d {input.checkv_db}/checkv-db-v1.5 >> {log} 2>&1    
    fi
    """

rule multiqc:
  input:
    fastp_json = expand("01.pre-processing/{sample}_fastp.json", sample = SAMPLES),
    qualimap_dir = expand("03.post-processing/mapping_evaluation/{sample}", sample = SAMPLES),
    quast_dir = expand("03.post-processing/assembly_evaluation/{sample}", sample = SAMPLES),
    prokka_dir = expand("10.annotation/prokka/{sample}", sample = SAMPLES),
    bakta_dir = expand("10.annotation/bakta/{sample}", sample = SAMPLES)
  output:
    multiqc_dir = directory("14.report"),
    multiqc_yaml = temp("14.report/multiqc_config.yaml")
  conda:
    "envs/multiqc.yaml"
  message:
    "--- MultiQC: Aggregate results. ---"
  log:
    "logs/multiqc.log"  
  shell:
    """
    printf "%s\n" "show_analysis_paths: False" "show_analysis_time: False" > {output.multiqc_yaml}

    multiqc --config {output.multiqc_yaml} -d -dd 1 {input.fastp_json} {input.qualimap_dir} {input.quast_dir} {input.prokka_dir} {input.bakta_dir} \
    --outdir {output.multiqc_dir} > {log} 2>&1
    """
